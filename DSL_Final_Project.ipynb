{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4",
      "collapsed_sections": [
        "ehaLTc0rhkjL"
      ],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "809646bb9a5d44a8a5b1762b0bd00623": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f7e45670d57c44bf9d1496f98691a1df",
              "IPY_MODEL_26196c0f82c049a5beacdb8a432b6772",
              "IPY_MODEL_cb1128ee929148e4b4afb6ffd581a974"
            ],
            "layout": "IPY_MODEL_57f516d450b14c888ebdc19e99fb7732"
          }
        },
        "f7e45670d57c44bf9d1496f98691a1df": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2177a84f0633426d88b5a1c590eac2b2",
            "placeholder": "​",
            "style": "IPY_MODEL_9b177ecaa6f94c6089776bb5c3a26c72",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "26196c0f82c049a5beacdb8a432b6772": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ee3bbecf93f34a39a125883de3452ebb",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a70f4e39659542dc943a777d38dceebf",
            "value": 4
          }
        },
        "cb1128ee929148e4b4afb6ffd581a974": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2126fb17384044b5b8f5adc1b927be45",
            "placeholder": "​",
            "style": "IPY_MODEL_d6fadc9e5dfe4b7ab441d196a0f7e5d7",
            "value": " 4/4 [00:08&lt;00:00,  1.94s/it]"
          }
        },
        "57f516d450b14c888ebdc19e99fb7732": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2177a84f0633426d88b5a1c590eac2b2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9b177ecaa6f94c6089776bb5c3a26c72": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ee3bbecf93f34a39a125883de3452ebb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a70f4e39659542dc943a777d38dceebf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2126fb17384044b5b8f5adc1b927be45": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d6fadc9e5dfe4b7ab441d196a0f7e5d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f2ac486f90e542c08f0b0dc3afc10b1b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f82fdde24ec54b24bcc1f837682a3689",
              "IPY_MODEL_81f887a1d3cc4bcfa5f5357c35bdbd92",
              "IPY_MODEL_d37f08c0b953437fbff58c46299efd7d"
            ],
            "layout": "IPY_MODEL_e2f9f8d4b3a04fdc88f7ba8b53beb7b7"
          }
        },
        "f82fdde24ec54b24bcc1f837682a3689": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fee3637ebdb348f798b015f3e10573ba",
            "placeholder": "​",
            "style": "IPY_MODEL_91c9e82d7fb74b00ab37ee4c9e67320c",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "81f887a1d3cc4bcfa5f5357c35bdbd92": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a44d92a75df94a048d4bb0b4e2bba31a",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3f742c5609cd403baa387e7514eebfd1",
            "value": 2
          }
        },
        "d37f08c0b953437fbff58c46299efd7d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b98c6ef481054be988eb1f7f3c0a4f42",
            "placeholder": "​",
            "style": "IPY_MODEL_442465cb2dfe450c9899ce9232b7a3ec",
            "value": " 2/2 [00:06&lt;00:00,  3.01s/it]"
          }
        },
        "e2f9f8d4b3a04fdc88f7ba8b53beb7b7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fee3637ebdb348f798b015f3e10573ba": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "91c9e82d7fb74b00ab37ee4c9e67320c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a44d92a75df94a048d4bb0b4e2bba31a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3f742c5609cd403baa387e7514eebfd1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b98c6ef481054be988eb1f7f3c0a4f42": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "442465cb2dfe450c9899ce9232b7a3ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "####INSTALLS"
      ],
      "metadata": {
        "id": "lej9dRZptqd6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install bitsandbytes\n",
        "!pip install --upgrade transformers # Install latest version of transformers\n",
        "!pip install --upgrade accelerate\n",
        "!pip install sentencepiece\n",
        "!pip install tqdm"
      ],
      "metadata": {
        "id": "Dxrm-rXNwVjv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import transformers\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        ")\n",
        "from torch.utils.data import Dataset"
      ],
      "metadata": {
        "id": "HkgsQ2KsVuzw"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install --quiet peft"
      ],
      "metadata": {
        "id": "s3i0AFTjYBxv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "228305cb-7b9b-4cfb-dfba-851aa0673c5b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/199.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.7/199.1 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.2/199.1 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.1/199.1 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import PeftModel, LoraConfig, TaskType, get_peft_model"
      ],
      "metadata": {
        "id": "h8XMi7WPZPmF"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install --quiet wandb"
      ],
      "metadata": {
        "id": "Ih-aUqX8_DJR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04a450b0-18bd-4612-892e-e6b7b374dc6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m31.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m266.8/266.8 kB\u001b[0m \u001b[31m30.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb"
      ],
      "metadata": {
        "id": "iH6bquXwnVVC"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wandb login"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NStOR8kWA1b1",
        "outputId": "4c1f0dac-1900-48c5-cbb2-c702c20d206e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mabhaymittal\u001b[0m (\u001b[33mabhaymittal-devops\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git config --global credential.helper store"
      ],
      "metadata": {
        "id": "1Jg8dJiSgYuz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login"
      ],
      "metadata": {
        "id": "qKJC1Z2IaXA1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62e37366-f2bf-4c88-8df9-9da0033fc9c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "\n",
            "    To login, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
            "Token: \n",
            "Add token as git credential? (Y/n) Y\n",
            "Token is valid (permission: write).\n",
            "Your token has been saved in your configured git credential helpers (store).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install transformers bitsandbytes accelerate"
      ],
      "metadata": {
        "id": "q8fIJ6hPg1d_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####LOADING LLAMA 2"
      ],
      "metadata": {
        "id": "sqIUybQvwspW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_model(\n",
        "    model_name: str = \"meta-llama/Llama-2-7b-chat-hf\", load_on_gpu: bool = True   #meta-llama/Meta-Llama-3-8B-Instruct\n",
        ") -> tuple[AutoModelForCausalLM, AutoTokenizer]:\n",
        "    if load_on_gpu:\n",
        "        config = BitsAndBytesConfig(\n",
        "            load_in_4bit=True,\n",
        "            bnb_4bit_use_double_quant=False,\n",
        "            bnb_4bit_quant_type=\"nf4\",\n",
        "            bnb_4bit_compute_dtype=torch.float16,\n",
        "        )\n",
        "\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_name, low_cpu_mem_usage=True, quantization_config=config\n",
        "        )\n",
        "    else:\n",
        "        print(\"Loading without quantization\")\n",
        "        model = AutoModelForCausalLM.from_pretrained(model_name, low_cpu_mem_usage=True)\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "        tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "    return model, tokenizer"
      ],
      "metadata": {
        "id": "7s5QJz_cghht"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the model & tokenizer\n",
        "model, tokenizer = load_model()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178,
          "referenced_widgets": [
            "f2ac486f90e542c08f0b0dc3afc10b1b",
            "f82fdde24ec54b24bcc1f837682a3689",
            "81f887a1d3cc4bcfa5f5357c35bdbd92",
            "d37f08c0b953437fbff58c46299efd7d",
            "e2f9f8d4b3a04fdc88f7ba8b53beb7b7",
            "fee3637ebdb348f798b015f3e10573ba",
            "91c9e82d7fb74b00ab37ee4c9e67320c",
            "a44d92a75df94a048d4bb0b4e2bba31a",
            "3f742c5609cd403baa387e7514eebfd1",
            "b98c6ef481054be988eb1f7f3c0a4f42",
            "442465cb2dfe450c9899ce9232b7a3ec"
          ]
        },
        "id": "JQaaoYQ2gkke",
        "outputId": "5aeaa8d4-876b-4e79-f5a0-f2adc1ddc241"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f2ac486f90e542c08f0b0dc3afc10b1b"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = [\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": \"You answer multiple choice questions with the correct letter answer. Your answer should be in this format: 'Correct Answer: {Letter}. Explanation: {Explanation}'\",\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"user\", \"content\": \"\"\"\"Continuous exposure to indoor fluorescent lights is beneficial to the health of hamsters with heart disease.One group of hamsters exposed to continuous exposure to fluorescent lights has an average lifespan that is 2.5% longer than another one of the same species but living in a black wall.\n",
        "Which of the following questions was the initial motivation for conducting the above experiment?\n",
        "A.Can hospital light therapy be proved to promote patient recovery?\n",
        "B.Which one lives longer, the hamster living under the light or the hamster living in the dark?\n",
        "C.What kind of illness does the hamster have?\n",
        "D.Do some hamsters need a period of darkness?\n",
        "        \"\"\"\n",
        "    },\n",
        "]\n",
        "\n",
        "input_ids = tokenizer.apply_chat_template(\n",
        "    query, add_generation_prompt=True, return_tensors=\"pt\"\n",
        ").to(model.device)\n",
        "\n",
        "terminators = [tokenizer.eos_token_id, tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")]\n",
        "\n",
        "outputs = model.generate(\n",
        "    input_ids,\n",
        "    max_new_tokens=256,\n",
        "    eos_token_id=terminators,\n",
        "    do_sample=True,\n",
        "    temperature=0.6,\n",
        "    top_p=0.9,\n",
        ")\n",
        "\n",
        "response = outputs[0][input_ids.shape[-1] :]\n",
        "\n",
        "print(tokenizer.decode(response, skip_special_tokens=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NPFr04dTilHd",
        "outputId": "13b54e2e-df2b-4997-9491-75776cdd54ab"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Correct Answer: B. Which one lives longer, the hamster living under the light or the hamster living in the dark?\n",
            "\n",
            "Explanation: The experiment was conducted to compare the lifespan of hamsters exposed to continuous exposure to fluorescent lights with those living in a dark environment. The initial motivation for the experiment was to determine whether exposure to indoor fluorescent lights has a positive effect on the health of hamsters with heart disease, rather than exploring other questions such as the effectiveness of hospital light therapy (A), the type of illness the hamsters have (C), or whether some hamsters need a period of darkness (D).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def inference(question):\n",
        "  query = [\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": \"You answer multiple choice questions with the correct letter answer. Your answer should be in this format: 'Correct Answer: {Letter}. Explanation: {Explanation}'\",\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"user\", \"content\": question\n",
        "    },\n",
        "  ]\n",
        "\n",
        "  input_ids = tokenizer.apply_chat_template(\n",
        "    query, add_generation_prompt=True, return_tensors=\"pt\"\n",
        "  ).to(model.device)\n",
        "\n",
        "  terminators = [tokenizer.eos_token_id, tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")]\n",
        "\n",
        "  outputs = model.generate(\n",
        "    input_ids,\n",
        "    max_new_tokens=256,\n",
        "    eos_token_id=terminators,\n",
        "    do_sample=True,\n",
        "    temperature=0.6,\n",
        "    top_p=0.9,\n",
        "  )\n",
        "\n",
        "  response = outputs[0][input_ids.shape[-1] :]\n",
        "\n",
        "  return tokenizer.decode(response, skip_special_tokens=True)"
      ],
      "metadata": {
        "id": "pMQP9n-0ruEe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def checkResponse(response, answer):\n",
        "  response = response.strip()\n",
        "  if(response.lower()[:15] == \"correct answer:\"):\n",
        "    if(response[16] == answer):\n",
        "      return 1\n",
        "    else:\n",
        "      return 0\n",
        "  return -1"
      ],
      "metadata": {
        "id": "qjgFs64GsPT3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm.auto import tqdm\n",
        "\n",
        "tqdm.pandas(desc=\"progress\")\n",
        "\n",
        "test_data = pd.read_csv(\"test.csv\")\n",
        "test_data[\"generated_response\"] = test_data.progress_apply(lambda row: inference(row[\"query\"]), axis=1)\n",
        "test_data[\"prediction\"] = test_data.progress_apply(lambda row: checkResponse(row[\"generated_response\"], row[\"response\"][0]), axis=1)\n"
      ],
      "metadata": {
        "id": "Ku8vSoDmpMGP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data.to_csv(\"results.csv\", index=False, header=True)"
      ],
      "metadata": {
        "id": "AJkI7BmtwNvK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####FINE-TUNING"
      ],
      "metadata": {
        "id": "l4HieJwlw8nH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_queries(tokenizer: AutoTokenizer, query: str, max_length: int = 1024) -> torch.Tensor:\n",
        "    query = [\n",
        "        {\"role\": \"system\", \"content\": \"You answer multiple choice questions with the correct letter answer. Your answer should be in this format: 'Correct Answer: {Letter}. Explanation: {Explanation}'\",\n",
        "},\n",
        "        {\"role\": \"user\", \"content\": query},\n",
        "    ]\n",
        "\n",
        "    input_ids = tokenizer.apply_chat_template(\n",
        "        query,\n",
        "        padding='max_length',\n",
        "        max_length=max_length,\n",
        "        add_generation_prompt=True,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "    return input_ids\n",
        "\n",
        "def tokenize_responses(tokenizer: AutoTokenizer, query: str, response: str, max_length: int = 1024) -> torch.Tensor:\n",
        "    response = [\n",
        "        {\"role\": \"system\", \"content\": \"You answer multiple choice questions with the correct letter answer. Your answer should be in this format: 'Correct Answer: {Letter}. Explanation: {Explanation}'\",\n",
        "},\n",
        "        {\"role\": \"user\", \"content\": query},\n",
        "        {\"role\": \"assistant\", \"content\": response},\n",
        "    ]\n",
        "\n",
        "    labels = tokenizer.apply_chat_template(\n",
        "        response,\n",
        "        padding='max_length',\n",
        "        max_length=max_length,\n",
        "        add_generation_prompt=True,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "    return labels"
      ],
      "metadata": {
        "id": "iO7ZKpzRrbbz"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RecipeDataset(Dataset):\n",
        "    def __init__(self, data: pd.DataFrame):\n",
        "        self.data = data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {\n",
        "            \"input_ids\": self.data.iloc[idx][\"input_ids\"][0],\n",
        "            \"labels\": self.data.iloc[idx][\"labels\"][0],\n",
        "        }"
      ],
      "metadata": {
        "id": "dOXXiW221Iey"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the model & tokenizer\n",
        "# model, tokenizer = load_model()\n",
        "\n",
        "# Add Lora Matrices\n",
        "lora_config = LoraConfig(\n",
        "    r=2,\n",
        "    lora_alpha=8,\n",
        "    target_modules=[\"q_proj\", \"v_proj\"],\n",
        "    lora_dropout=0.2,\n",
        "    bias=\"none\",\n",
        "    task_type=TaskType.CAUSAL_LM,\n",
        ")\n",
        "\n",
        "peft_model = get_peft_model(model, lora_config)"
      ],
      "metadata": {
        "id": "47YOI7ft1a0A"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the number of trainable parameters\n",
        "print(f\"Number of trainable parameters: {sum(p.numel() for p in peft_model.parameters() if p.requires_grad)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kR00gsT91-EQ",
        "outputId": "622583dc-9995-49e1-a238-0e67daeb46fc"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of trainable parameters: 1048576\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load in the data\n",
        "train_data = pd.read_csv(\"train.csv\")\n",
        "eval_data = pd.read_csv(\"eval.csv\")\n",
        "\n",
        "# Take the first 100 samples for training\n",
        "train_data = train_data.head(100)\n",
        "\n",
        "# Take the first 10 samples for evaluation\n",
        "eval_data = eval_data.head(10)"
      ],
      "metadata": {
        "id": "u5TtYsyz1_T5"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize the data\n",
        "train_data[\"input_ids\"] = train_data.apply(lambda row: tokenize_queries(tokenizer, row[\"query\"]), axis=1)\n",
        "train_data[\"labels\"] = train_data.apply(lambda row: tokenize_responses(tokenizer, row[\"query\"], row[\"response\"]), axis=1)\n",
        "\n",
        "eval_data[\"input_ids\"] = eval_data.apply(lambda row: tokenize_queries(tokenizer, row[\"query\"]), axis=1)\n",
        "eval_data[\"labels\"] = eval_data.apply(lambda row: tokenize_responses(tokenizer, row[\"query\"], row[\"response\"]), axis=1)\n",
        "\n",
        "# Drop unecessary columns\n",
        "train_data = train_data.drop(columns=\"query\").drop(columns=\"response\")\n",
        "eval_data = eval_data.drop(columns=\"query\").drop(columns=\"response\")"
      ],
      "metadata": {
        "id": "D_TTW5NZ2H38"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.init(\n",
        "    project=\"dsl-final-project-1\"\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        },
        "id": "1_rtQHfnnbTG",
        "outputId": "d8e575b9-8b3f-4d53-89c1-0505f837d3b2"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mabhaymittal\u001b[0m (\u001b[33mabhaymittal-devops\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.16.6"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20240427_234514-449eaqld</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/abhaymittal-devops/dsl-final-project-1/runs/449eaqld' target=\"_blank\">pleasant-sun-6</a></strong> to <a href='https://wandb.ai/abhaymittal-devops/dsl-final-project-1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/abhaymittal-devops/dsl-final-project-1' target=\"_blank\">https://wandb.ai/abhaymittal-devops/dsl-final-project-1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/abhaymittal-devops/dsl-final-project-1/runs/449eaqld' target=\"_blank\">https://wandb.ai/abhaymittal-devops/dsl-final-project-1/runs/449eaqld</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/abhaymittal-devops/dsl-final-project-1/runs/449eaqld?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
            ],
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x799cec61a410>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup trainer\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./checkpoints\",\n",
        "    per_device_train_batch_size=1,\n",
        "    per_device_eval_batch_size=1,\n",
        "    learning_rate=1e-3,\n",
        "    num_train_epochs=5,\n",
        "    logging_steps=1,\n",
        "    save_steps=10,\n",
        "    log_level=\"info\",\n",
        "    report_to=\"wandb\"\n",
        ")\n",
        "\n",
        "trainer = Trainer(model=peft_model,\n",
        "    args=training_args,\n",
        "    train_dataset=RecipeDataset(train_data),\n",
        "    eval_dataset=RecipeDataset(eval_data)\n",
        ")"
      ],
      "metadata": {
        "id": "J89eeBA52dbF"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now train\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "Z67wdwcQ2hdl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "Wv7sErdIYXQD"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####LORA MODEL"
      ],
      "metadata": {
        "id": "ehaLTc0rhkjL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_lora_model(\n",
        "    adapter_name: str,\n",
        "    model_name: str = \"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
        ") -> tuple[AutoModelForCausalLM, AutoTokenizer]:\n",
        "    config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_use_double_quant=False,\n",
        "        bnb_4bit_quant_type=\"nf4\",\n",
        "        bnb_4bit_compute_dtype=torch.float16,\n",
        "    )\n",
        "\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name, low_cpu_mem_usage=True, quantization_config=config\n",
        "    )\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "    model = PeftModel.from_pretrained(model, adapter_name)\n",
        "\n",
        "    return model, tokenizer"
      ],
      "metadata": {
        "id": "uJ2xqz2Zhm9o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the model & tokenizer\n",
        "model, tokenizer = load_lora_model(\"checkpoints/checkpoint-360\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "809646bb9a5d44a8a5b1762b0bd00623",
            "f7e45670d57c44bf9d1496f98691a1df",
            "26196c0f82c049a5beacdb8a432b6772",
            "cb1128ee929148e4b4afb6ffd581a974",
            "57f516d450b14c888ebdc19e99fb7732",
            "2177a84f0633426d88b5a1c590eac2b2",
            "9b177ecaa6f94c6089776bb5c3a26c72",
            "ee3bbecf93f34a39a125883de3452ebb",
            "a70f4e39659542dc943a777d38dceebf",
            "2126fb17384044b5b8f5adc1b927be45",
            "d6fadc9e5dfe4b7ab441d196a0f7e5d7"
          ]
        },
        "id": "Qs2Gr1L-hst0",
        "outputId": "9613425e-109e-4588-e8ef-25655558d304"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/e5e23bbe8e749ef0efcf16cad411a7d23bd23298/config.json\n",
            "Model config LlamaConfig {\n",
            "  \"_name_or_path\": \"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
            "  \"architectures\": [\n",
            "    \"LlamaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"eos_token_id\": 128001,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 14336,\n",
            "  \"max_position_embeddings\": 8192,\n",
            "  \"model_type\": \"llama\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"pretraining_tp\": 1,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 500000.0,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.40.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 128256\n",
            "}\n",
            "\n",
            "Overriding torch_dtype=None with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in 8-bit or 4-bit. Pass your own torch_dtype to specify the dtype of the remaining non-linear layers or pass torch_dtype=torch.float16 to remove this warning.\n",
            "The device_map was not initialized. Setting device_map to {'':torch.cuda.current_device()}. If you want to use the model for inference, please set device_map ='auto' \n",
            "loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/e5e23bbe8e749ef0efcf16cad411a7d23bd23298/model.safetensors.index.json\n",
            "Instantiating LlamaForCausalLM model under default dtype torch.float16.\n",
            "Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"eos_token_id\": 128001\n",
            "}\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "809646bb9a5d44a8a5b1762b0bd00623"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint weights were used when initializing LlamaForCausalLM.\n",
            "\n",
            "All the weights of LlamaForCausalLM were initialized from the model checkpoint at meta-llama/Meta-Llama-3-8B-Instruct.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n",
            "loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/e5e23bbe8e749ef0efcf16cad411a7d23bd23298/generation_config.json\n",
            "Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"do_sample\": true,\n",
            "  \"eos_token_id\": [\n",
            "    128001,\n",
            "    128009\n",
            "  ],\n",
            "  \"max_length\": 4096,\n",
            "  \"temperature\": 0.6,\n",
            "  \"top_p\": 0.9\n",
            "}\n",
            "\n",
            "loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/e5e23bbe8e749ef0efcf16cad411a7d23bd23298/tokenizer.json\n",
            "loading file added_tokens.json from cache at None\n",
            "loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/e5e23bbe8e749ef0efcf16cad411a7d23bd23298/special_tokens_map.json\n",
            "loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/e5e23bbe8e749ef0efcf16cad411a7d23bd23298/tokenizer_config.json\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = [\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": \"Greet me in a few words.\",\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"user\", \"content\": \"Hello!\"\n",
        "    },\n",
        "]\n",
        "\n",
        "input_ids = tokenizer.apply_chat_template(\n",
        "    query, add_generation_prompt=True, return_tensors=\"pt\"\n",
        ").to(model.device)\n",
        "\n",
        "terminators = [tokenizer.eos_token_id, tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")]\n",
        "\n",
        "outputs = model.generate(\n",
        "    input_ids,\n",
        "    max_new_tokens=256,\n",
        "    eos_token_id=terminators,\n",
        "    do_sample=True,\n",
        "    temperature=0.6,\n",
        "    top_p=0.9,\n",
        ")\n",
        "\n",
        "response = outputs[0][input_ids.shape[-1] :]\n",
        "\n",
        "print(tokenizer.decode(response, skip_special_tokens=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZkwozCluhut0",
        "outputId": "8a8f281b-5b1c-40c8-d654-54b00c3217d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "assistantassistant\n"
          ]
        }
      ]
    }
  ]
}